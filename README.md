# Multilayer Perceptron (MLP) - Clasificaci√≥n MNIST y An√°lisis de Sentimientos

Este proyecto implementa un perceptr√≥n multicapa desde cero para dos tareas principales:
1. **Clasificaci√≥n de d√≠gitos manuscritos** del dataset MNIST (clasificaci√≥n multiclase)
2. **An√°lisis de sentimientos** en rese√±as de pel√≠culas (clasificaci√≥n binaria)

El proyecto incluye experimentos exhaustivos con diferentes funciones de activaci√≥n, t√©cnicas de regularizaci√≥n, y optimizaci√≥n del learning rate.

## Descripci√≥n

El proyecto incluye dos implementaciones completas de redes neuronales multicapa:

### Clasificaci√≥n MNIST (Multiclase)
- **50 neuronas en la capa oculta**
- **10 neuronas de salida** (para los 10 d√≠gitos: 0-9)
- **Funciones de activaci√≥n**: ReLU, LeakyReLU, Sigmoid
- **Experimentos con estandarizaci√≥n** de datos
- **Learning rate adaptativo** con decaimiento lineal inverso
- **An√°lisis comparativo** de diferentes configuraciones

### An√°lisis de rese√±as (Binario)
- **Arquitectura configurable** (32, 64, 128 neuronas ocultas)
- **1 neurona de salida** (positivo/negativo)
- **Procesamiento de texto** con Bag-of-Words
- **B√∫squeda autom√°tica de hiperpar√°metros**
- **M√©tricas especializadas** para clasificaci√≥n binaria
- **Matriz de confusi√≥n** y an√°lisis de errores

## Estructura del proyecto

```
multilayer_perceptron/
‚îú‚îÄ‚îÄ üìÅ Implementaci√≥n MNIST (Multiclase)
‚îÇ   ‚îú‚îÄ‚îÄ MLP.py                          # Implementaci√≥n original del MLP
‚îÇ   ‚îú‚îÄ‚îÄ MLP_mod.py                      # Versi√≥n modificada con m√∫ltiples activaciones
‚îÇ   ‚îú‚îÄ‚îÄ activation_functions.py         # Funciones de activaci√≥n originales
‚îÇ   ‚îú‚îÄ‚îÄ activation_functions_mod.py     # Funciones de activaci√≥n extendidas
‚îÇ   ‚îú‚îÄ‚îÄ batch_generator.py             # Generador de mini-batches original
‚îÇ   ‚îú‚îÄ‚îÄ batch_generator_mod.py         # Versi√≥n modificada
‚îÇ   ‚îú‚îÄ‚îÄ load_dataset.py                # Carga del dataset MNIST
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py                     # M√©tricas originales
‚îÇ   ‚îú‚îÄ‚îÄ metrics_mod.py                 # M√©tricas modificadas
‚îÇ   ‚îú‚îÄ‚îÄ train.py                       # Script de entrenamiento original
‚îÇ   ‚îú‚îÄ‚îÄ train_mod.py                   # Script de entrenamiento modificado
‚îÇ   ‚îú‚îÄ‚îÄ utils.py                       # Utilidades originales
‚îÇ   ‚îú‚îÄ‚îÄ utils_mod.py                   # Utilidades con estandarizaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ run_model.py                   # Script principal de experimentos MNIST
‚îÇ   ‚îî‚îÄ‚îÄ adaptive_lr_experiments.py     # Experimentos con LR adaptativo
‚îÇ
‚îú‚îÄ‚îÄ üìÅ An√°lisis de Sentimientos (Binario)
‚îÇ   ‚îú‚îÄ‚îÄ MLP_binary.py                  # MLP para clasificaci√≥n binaria
‚îÇ   ‚îú‚îÄ‚îÄ text_processor.py              # Procesamiento de texto y BOW
‚îÇ   ‚îú‚îÄ‚îÄ metrics_binary.py              # M√©tricas especializadas binarias
‚îÇ   ‚îú‚îÄ‚îÄ movie_sentiment.py             # Script principal de sentimientos
‚îÇ   ‚îî‚îÄ‚îÄ quick_sentiment_test.py        # Test r√°pido simplificado
‚îÇ
‚îú‚îÄ‚îÄ üìÅ Datos
‚îÇ   ‚îî‚îÄ‚îÄ dataset/
‚îÇ       ‚îî‚îÄ‚îÄ test.csv                   # Dataset de rese√±as de pel√≠culas
‚îÇ
‚îú‚îÄ‚îÄ üìÅ Resultados
‚îÇ   ‚îî‚îÄ‚îÄ figures/                       # Gr√°ficas generadas
‚îÇ       ‚îú‚îÄ‚îÄ accuracy_comparison.png    # Comparaci√≥n de accuracy MNIST
‚îÇ       ‚îú‚îÄ‚îÄ loss_comparison.png        # Comparaci√≥n de loss MNIST
‚îÇ       ‚îú‚îÄ‚îÄ lr_comparison_*.png        # Comparaci√≥n de learning rates
‚îÇ       ‚îú‚îÄ‚îÄ sentiment_training_curves.png  # Curvas de entrenamiento sentimientos
‚îÇ       ‚îú‚îÄ‚îÄ sentiment_confusion_matrix.png # Matriz de confusi√≥n
‚îÇ       ‚îî‚îÄ‚îÄ sentiment_results.png      # Resultados completos sentimientos
‚îÇ
‚îî‚îÄ‚îÄ __pycache__/                       # Archivos compilados de Python
```

## Experimentos realizados

### Experimento 1: Comparaci√≥n de Configuraciones MNIST

El proyecto eval√∫a el impacto de diferentes configuraciones en el rendimiento del modelo:

| Experimento | Estandarizaci√≥n | Learning Rate | Funci√≥n de Activaci√≥n |
|------------|----------------|---------------|----------------------|
| No_std_0.1_ReLU | No | 0.1 | ReLU |
| Std_0.1_ReLU | S√≠ | 0.1 | ReLU |
| Std_0.5_ReLU | S√≠ | 0.5 | ReLU |
| Std_0.7_ReLU | S√≠ | 0.7 | ReLU |
| Std_0.1_LeakyReLU | S√≠ | 0.1 | LeakyReLU |
| Std_0.5_LeakyReLU | S√≠ | 0.5 | LeakyReLU |
| Std_0.7_LeakyReLU | S√≠ | 0.7 | LeakyReLU |

### Experimento 2: Learning Rate Adaptativo

Implementaci√≥n de decaimiento lineal inverso:
```
Œ∑_t = c1 / (iteraci√≥n + c2)
```

**Par√°metros evaluados:**
- c1 = 0.1 (learning rate inicial)
- c2 ‚àà {5, 10, 20, 50} (velocidad de decaimiento)

**Comparaci√≥n:** LR est√°tico vs LR adaptativo

### üé¨ Experimento 3: An√°lisis de rese√±as

**B√∫squeda autom√°tica de hiperpar√°metros:**
- **Neuronas ocultas**: [32, 64, 128]
- **Learning rate**: [0.001, 0.01, 0.1]
- **Funciones de activaci√≥n**: [ReLU, LeakyReLU]
- **Tama√±o de batch**: [32, 64, 128]
- **√âpocas**: [50, 100]

### Par√°metros generales MNIST
- **√âpocas de entrenamiento**: 150
- **Divisi√≥n de datos**: 80% entrenamiento, 20% prueba
- **Validaci√≥n**: 20% del conjunto de entrenamiento
- **Tama√±o de mini-batch**: 100
- **Inicializaci√≥n de pesos**: Distribuci√≥n normal (Œº=0, œÉ=0.1)

## Instalaci√≥n y uso

### Requisitos
```bash
pip install numpy matplotlib scikit-learn pandas seaborn
```

### Ejecuci√≥n MNIST
```bash
# Ejecutar todos los experimentos MNIST
python run_model.py

# Experimentos de learning rate adaptativo
python adaptive_lr_experiments.py

# Entrenamiento b√°sico
python train.py
```

### Ejecuci√≥n an√°lisis de sentimientos
```bash
# Test r√°pido (recomendado para empezar)
python quick_sentiment_test.py

# B√∫squeda completa de hiperpar√°metros (m√°s lento)
python movie_sentiment.py
```

## Funciones de activaci√≥n implementadas

### 1. ReLU (Rectified Linear Unit)
```python
f(x) = max(0, x)
f'(x) = 1 if x > 0, else 0
```

### 2. LeakyReLU
```python
f(x) = x if x > 0, else Œ±*x  (Œ± = 0.01)
f'(x) = 1 if x > 0, else Œ±
```

### 3. Sigmoid
```python
f(x) = 1 / (1 + e^(-x))
f'(x) = f(x) * (1 - f(x))
```

## M√©tricas evaluadas

### MNIST (Multiclase)
- **Accuracy**: Porcentaje de predicciones correctas
- **MSE Loss**: Error cuadr√°tico medio
- **Tiempo de entrenamiento**: Duraci√≥n del proceso de entrenamiento
- **Curvas de aprendizaje**: Evoluci√≥n de accuracy y loss por √©poca
- **Learning rate evolution**: Seguimiento del LR adaptativo

### Sentimientos (Binario)
- **Accuracy**: Porcentaje de predicciones correctas
- **Precision**: Verdaderos positivos / (Verdaderos + Falsos positivos)
- **Recall**: Verdaderos positivos / (Verdaderos positivos + Falsos negativos)
- **F1-Score**: Media arm√≥nica de precision y recall
- **Binary Cross Entropy Loss**: Funci√≥n de p√©rdida especializada
- **Matriz de Confusi√≥n**: Visualizaci√≥n de errores de clasificaci√≥n

## Caracter√≠sticas t√©cnicas

### Estandarizaci√≥n de satos
```python
X_standardized = (X - Œº) / œÉ
```
Donde:
- Œº = media del conjunto de entrenamiento
- œÉ = desviaci√≥n est√°ndar del conjunto de entrenamiento

### Learning Rate adaptativo
```python
Œ∑_t = c1 / (iteration + c2)
```
Donde:
- Œ∑_t = learning rate en la iteraci√≥n t
- c1 = learning rate inicial
- c2 = controla la velocidad de decaimiento

### Arquitectura MNIST
- **Capa de entrada**: 784 neuronas (28√ó28 p√≠xeles)
- **Capa oculta**: 50 neuronas con funci√≥n de activaci√≥n configurable
- **Capa de salida**: 10 neuronas con activaci√≥n sigmoid
- **Funci√≥n de p√©rdida**: Error cuadr√°tico medio (MSE)

### Arquitectura rese√±a
- **Capa de entrada**: Variable (tama√±o del vocabulario)
- **Capa oculta**: Configurable (32, 64, 128 neuronas)
- **Capa de salida**: 1 neurona con activaci√≥n sigmoid
- **Funci√≥n de p√©rdida**: Binary Cross Entropy
- **Representaci√≥n**: Bag-of-Words (BOW)

### Procesamiento de texto
- **Limpieza**: Conversi√≥n a min√∫sculas, eliminaci√≥n de caracteres especiales
- **Vocabulario**: Top 1000-5000 palabras m√°s frecuentes
- **Tokens especiales**: `<PAD>`, `<UNK>` para padding y palabras desconocidas
- **Caracter√≠sticas**: Matriz de frecuencias de palabras

### Algoritmo de entrenamiento
- **Optimizador**: Descenso de gradiente por mini-batches
- **Backpropagation**: C√°lculo de gradientes capa por capa
- **Actualizaci√≥n de pesos**: W = W - Œ∑‚àáW
- **Inicializaci√≥n**: Xavier/He para mejor convergencia

## Resultados esperados

### Experimentos MNIST

Los experimentos permiten analizar:

1. **Impacto de la estandarizaci√≥n**: 
   - Los datos estandarizados facilitan la convergencia
   - Mejora la estabilidad del entrenamiento
   - Reducci√≥n del tiempo de convergencia

2. **Efecto del learning rate**:
   - Valores muy altos pueden causar inestabilidad
   - Valores muy bajos ralentizan la convergencia
   - LR adaptativo mejora la convergencia final

3. **Comparaci√≥n de funciones de activaci√≥n**:
   - ReLU: R√°pida convergencia pero riesgo de "neuronas muertas"
   - LeakyReLU: M√°s robusta que ReLU, evita neuronas muertas
   - Sigmoid: Convergencia m√°s lenta, riesgo de gradiente desvaneciente

4. **Learning rate adaptativo vs est√°tico**:
   - Convergencia m√°s suave y estable
   - Mejor accuracy final
   - Reducci√≥n de oscilaciones en el entrenamiento

### üé¨ Experimentos Sentimientos

1. **Procesamiento de texto**:
   - Importancia del tama√±o del vocabulario
   - Efectividad de BOW para sentimientos
   - Limpieza de texto mejora resultados

2. **Arquitectura √≥ptima**:
   - N√∫mero de neuronas ocultas vs overfitting
   - Balance entre complejidad y generalizaci√≥n
   - Funciones de activaci√≥n para texto

3. **Hiperpar√°metros**:
   - Learning rate √≥ptimo para clasificaci√≥n binaria
   - Tama√±o de batch vs estabilidad
   - N√∫mero de √©pocas vs convergencia

### M√©tricas t√≠picas esperadas
- **MNIST**: 85-95% accuracy (dependiendo de configuraci√≥n)
- **Sentimientos**: 70-85% accuracy (dependiendo de dataset y preprocesamiento)

## Archivos principales

### Implementaci√≥n MNIST
- `MLP_mod.py`: Clase principal del perceptr√≥n multicapa
- `activation_functions_mod.py`: Funciones de activaci√≥n y derivadas
- `utils_mod.py`: Utilidades incluyendo estandarizaci√≥n y LR adaptativo
- `metrics_mod.py`: C√°lculo de m√©tricas de evaluaci√≥n
- `run_model.py`: Script principal para experimentos b√°sicos
- `adaptive_lr_experiments.py`: Experimentos con learning rate adaptativo

### Implementaci√≥n sentimientos
- `MLP_binary.py`: Red neuronal para clasificaci√≥n binaria
- `text_processor.py`: Procesamiento de texto y creaci√≥n de BOW
- `metrics_binary.py`: M√©tricas especializadas para clasificaci√≥n binaria
- `movie_sentiment.py`: Script principal con b√∫squeda de hiperpar√°metros
- `quick_sentiment_test.py`: Test r√°pido simplificado

### Scripts de soporte
- `batch_generator_mod.py`: Generaci√≥n de mini-batches optimizada
- `train_mod.py`: Funci√≥n de entrenamiento

## Objetivos del proyecto

1. **Implementar** un MLP desde cero usando solo NumPy
2. **Comparar** diferentes funciones de activaci√≥n (ReLU, LeakyReLU, Sigmoid)
3. **Evaluar** el impacto de la estandarizaci√≥n de datos
4. **Analizar** el efecto de diferentes learning rates (est√°tico vs adaptativo)
5. **Desarrollar** sistema de clasificaci√≥n binaria para an√°lisis de sentimientos
6. **Optimizar** hiperpar√°metros autom√°ticamente
7. **Visualizar** curvas de aprendizaje y m√©tricas
8. **Generar** an√°lisis comparativo de resultados
9. **Implementar** procesamiento de texto con BOW
10. **Crear** matrices de confusi√≥n y an√°lisis de errores

## Conceptos implementados

### Redes Neuronales
- **Forward Propagation**: C√°lculo de salidas capa por capa
- **Backward Propagation**: C√°lculo y propagaci√≥n de gradientes
- **Mini-batch Gradient Descent**: Optimizaci√≥n por lotes peque√±os
- **Adaptive Learning Rate**: Decaimiento lineal inverso del LR
- **Weight Initialization**: Xavier/He para mejor convergencia

### Procesamiento de datos
- **One-hot Encoding**: Codificaci√≥n de etiquetas categ√≥ricas
- **Data Standardization**: Normalizaci√≥n estad√≠stica de features
- **Text Preprocessing**: Limpieza y tokenizaci√≥n de texto
- **Bag-of-Words**: Representaci√≥n vectorial de documentos
- **Vocabulary Building**: Construcci√≥n de diccionarios de palabras

### Evaluaci√≥n y validaci√≥n
- **Cross-validation**: Divisi√≥n de datos para validaci√≥n
- **Binary Classification Metrics**: Precision, Recall, F1-Score
- **Confusion Matrix**: An√°lisis detallado de errores
- **Hyperparameter Search**: B√∫squeda autom√°tica de configuraciones
- **Learning Curves**: Visualizaci√≥n del proceso de aprendizaje

### Optimizaci√≥n
- **Gradient Descent**: Optimizaci√≥n de par√°metros
- **Learning Rate Scheduling**: Ajuste din√°mico del LR
- **Regularization**: T√©cnicas para evitar overfitting
- **Early Stopping**: Prevenci√≥n de sobreentrenamiento

## Contribuciones

Este proyecto fue desarrollado como parte de un estudio comparativo de t√©cnicas de deep learning, implementando algoritmos fundamentales desde cero para comprender mejor su funcionamiento interno.

### Aspectos educativos
- **Implementaci√≥n desde cero**: Sin frameworks de alto nivel para comprender los fundamentos
- **Comparaci√≥n exhaustiva**: M√∫ltiples configuraciones y enfoques
- **Visualizaci√≥n completa**: Gr√°ficas y m√©tricas detalladas
- **An√°lisis profundo**: Discusi√≥n de resultados y observaciones
- **Aplicaci√≥n pr√°ctica**: Dos dominios diferentes (visi√≥n y texto)

### Caracter√≠sticas t√©cnicas
- **C√≥digo modular**: Separaci√≥n clara de responsabilidades
- **Experimentaci√≥n sistem√°tica**: B√∫squeda estructurada de hiperpar√°metros
- **Reproducibilidad**: Seeds fijos para resultados consistentes
- **Documentaci√≥n**: Comentarios y explicaciones detalladas

**¬°Feliz experimentaci√≥n con redes neuronales!**
